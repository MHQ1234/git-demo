{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动手深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 6, 7]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义张量（数组）\n",
    "x = torch.tensor([[1,2,3], [4,6,7]], dtype = torch.float32) # 相当于np.array()，极其相似，但是类型不同\n",
    "x = torch.arange(12) # 跟np.arange()效果相同\n",
    "torch.zeros(3,4) # 定义指定形状的数组，类似的numpy操作都可实现\n",
    "# tensor的四则运算规则同数组的也是相同的性质\n",
    "# 0维度张量，专门0维度张量的原因是：GPU中不能运行普通的1等标量，只能识别tensor(1)这种0维度张量\n",
    "torch.tensor([1]) # 1维张量\n",
    "torch.tensor(1) # 0维张量，也就是标量\n",
    "# 高维张量：类似于高维numpy数组，不做具体介绍\n",
    "\n",
    "\n",
    "# 查看张量信息\n",
    "x.ndim # 返回维度数（有几个维度）\n",
    "x.shape # 返回形状（每个层维度的个数组合）\n",
    "len(x) # 返回张量多个维度中最高层级维度的个数\n",
    "x.numel() # 最底层元素总数量\n",
    "x.dtype # 查看张量数据类型\n",
    "\n",
    "\n",
    "# 张量的形变\n",
    "x = torch.tensor([[1,2,3,4],[5,6,7,8],[9,9,9,9]])\n",
    "x.flatten() # 将任意维度张量拉伸为1维张量，包括0维张量\n",
    "x.reshape(3, 4) # 改变张量为指定形状，跟np.reshape相同\n",
    "x.reshape(12) # 变为1维张量，或者写作x.reshape(12,)，但是x.shape(12,1)是二维张量\n",
    "# reshape的参数输入对应于.shape的结果，他们的意思是一样的\n",
    "\n",
    "\n",
    "# 张量的索引\n",
    "# 1. 与数组的索引相同，但要注意的是，索引找到的最基本的元素都是0维张量，不是基本的标量，比如索引结果是tensor(1),而不是1\n",
    "# 2. 切片操作基本与numpy数组的索引相同，但有点区别，如下举例\n",
    "x[1:8:2] # 从第2个元素到第8个元素隔2步索引一次\n",
    "x[1::2] # 从第2个到尾，隔2步索引一次\n",
    "x[::2] # 从头到尾，balabala\n",
    "# 3. x[8:1:-2] 这种操作不行，torch不支持倒着索引\n",
    "# 4. 高维张量索引，与numpy的也没什么\n",
    "# 5. index_select(,,)索引：本质上任何索引都可以x[,,,...]解决，但是当维度很高的时候，有些操作会很麻烦。\n",
    "# 比如我想对一个有100个维度的张量做操作：取出第50个维中的前两行，正常写法是x[:,:,:...,[0,1],:,:,...],过于麻烦，但使用index_select就很方便\n",
    "# torch.index_select用于取出某一个特定维度的某几个列数据\n",
    "x = torch.arange(24).reshape(2,3,4)\n",
    "torch.index_select(x, 2, torch.tensor(0)) # 用法：x是目标张量，第二个参数是第几个维度，第三个参数是索引值，可以是多个，但必须是tensor型，不能是列表或数组型\n",
    "torch.index_select(x, 1, torch.tensor([0])) # 只索引一个单位时，可以用0维或者1维张量\n",
    "torch.index_select(x, 0, torch.tensor([0,1])) # 索引多个单位\n",
    "# 注：torch.tensor([0:1])是不对的！，不能写成切片，第三个参数本质是一个tensor类型的数据！\n",
    "\n",
    "\n",
    "# 张量的切分\n",
    "# 1. view()方法，张量的视图\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = x.view(4,3) # 此处view的作用类似于reshape。看上去y和x是不同的对象，但其实指向同一个存储空间，x和y可以看做一个数据的不同表现形式，改变x后Y也会改变，但仍然是不同形状\n",
    "\n",
    "# 2. torch.chunk(t, 4, dim = 0)方法：将张量进行切分的函数。t是要被切分的张量，4是等切分成4份，dim是从哪个维度上切，0是默认最高维，以此类推\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = torch.chunk(x, 4, dim = 1) # y是将x按列切分四等份的结果\n",
    "# 注：关于输出结果：是一个元组，其元素是切分后的各个单位；切分后的个体也是tensor对象；切分后每个个体仍然保持没切分的维度，不会降维；\n",
    "# 返回的结果也是视图，不是新对象；默认是等分，但如果不能等分也不会报错，会寻找近似等分，或者不等分\n",
    "\n",
    "# 3. torch.split(t, [a1,a2,a3...], dim = ) 与chunk相似，但有区别，第二个参数可以输入一个列表，表示自定义切分分配方案，要求总和等于对应维度的分量个数\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = torch.split(x, 4, dim = 1) # 此时的split与chunk效果相同\n",
    "z = torch.split(x, [1,1,2], dim = 1) # 按照列进行1、1、2份数分配方案的切分\n",
    "\n",
    "\n",
    "# 张量的合并\n",
    "# 1. torch.cat([x,y], dim = 1)，张量拼接操作，类似于numpy的vstack和hstack，以及pandas的concat；dim是选择按哪个维度拼接，默认是0\n",
    "x = torch.zeros((4,4))\n",
    "y = torch.zeros((4,4))\n",
    "torch.cat([x,y], dim = 1) # 按列拼接（横着拼）\n",
    "\n",
    "# 2. torch.stack([x,y]) 张量堆叠函数，并不是拼接，而是把相同形状的多个张量堆到一起，整体升高一个维度（类似于list的append）\n",
    "# 注：x和y的shape必须完全相同才行，否则会报错\n",
    "\n",
    "\n",
    "# 张量的广播\n",
    "# 1. 相同维度相同形状的两个张量广播，对应位置进行计算即可\n",
    "# 2. 相同维度不同形状的两个张量广播，要求在导致形状不同的每个维度上，两个张量中至少有一个在该维度的分量数只有1才行\n",
    "# 3. 不同维度的两个张量广播，低纬度的张量先升维到与高纬度张量相同维度，然后再按照同纬度张量广播规则判定即可\n",
    "# 不同维度广播运算举例：x.shape = (2,1)， y.shape = (3,2,4)。x根据情况选择升维成(1,2,1)，然后(1,2,1)与(3,2,4)是课广播的\n",
    "\n",
    "\n",
    "# 基本并行运算（axis与dim都可以使用，效果相同)\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "x.sum() # 对所有元素求和\n",
    "x.sum(dim = 0) # 对所有行求和，返回长度为3的向量\n",
    "x.sum(dim = 1) # 对所有列求和，返回长度为4的向量\n",
    "x,sum(dim = [0, 1]) # 对于多维数组，对其中某几维进行求和，这里只有2维所以结果跟sum()一样\n",
    "# 排序运算\n",
    "x = torch.randn(12).reshape(3,4) \n",
    "y = x.sort(dim = 1, descending = False).values # 将x按照某个维度升序（默认）排序，这个维度的每个分量都会独立进行排序（打乱对应关系），默认是0；返回的内容比较复杂，.values后是值\n",
    "\n",
    "\n",
    "\n",
    "# 矩阵的基本运算\n",
    "t1 = torch.arange(1,7).reshape(2,3).float()\n",
    "t2 = torch.arange(1,10).reshape(3,3).float()\n",
    "t = torch.arange(3).float()\n",
    "torch.t(t1) # 矩阵转置 \n",
    "torch.eye(3) # 单位阵创建\n",
    "torch.diag(t1.reshape(6)) # 将1维张量变成对角阵元素\n",
    "torch.dot(t,t) # 向量内积运算，dot只支持向量运算\n",
    "torch.mm(t1,t2) # 矩阵乘法，t1 * t2是对应元素相乘\n",
    "torch.mv(t1,t) ### 注意：特殊的矩阵与向量相乘的方法，并非是矩阵运算，而是把矩阵的每行当成一个向量分别与某个向量进行内积，返回各自内积结果组成的1维向量\n",
    "torch.mv(t2, t.reshape(3,1)) # 会报错，不能竖着来\n",
    "torch.mm(t1, t.reshape(3,1)) # 作为矩阵运算可以\n",
    "\n",
    "# 矩阵线性代数运算（不举例了，懒）\n",
    "torch.trace() # 矩阵的迹\n",
    "torch.matrix_rank() # 矩阵的秩\n",
    "torch.det() # 矩阵行列式计算\n",
    "torch.inverse() # 矩阵求逆\n",
    "torch.lstsq(y, X) # 最小二乘结果，y是因变量，X是带1列向量的变量矩阵\n",
    "torch.linalg.norm(t) # 求张量内所有元素平方和开根号（必须是float类型）\n",
    "torch.linalg.norm(t,1) # 求张量内所有元素绝对值和（必须是float类型）\n",
    "\n",
    "\n",
    "\n",
    "# 微分运算\n",
    "a = torch.tensor(1., requires_grad = True) # 创建一个张量的时候可以设置这个张量是否可以进行微分运算，默认是False（必须针对浮点型）\n",
    "b = torch.tensor(1., requires_grad = True)\n",
    "a.requires_grad # 用于查看这个张量是否可以微分\n",
    "# a.requires_grad = False # 可以设置为False\n",
    "sse = torch.pow((2-a-b),2) + torch.pow((4-3*a-b),2)\n",
    "torch.autograd.grad(sse, [a, b]) # 这个是对sse这个函数进行偏导运算，a和b就是变量，但是要先赋值，然后返回的是sse在a和b两个方向的导数在他们各自取值下的导数值\n",
    "\n",
    "x = torch.tensor(1., requires_grad = True)\n",
    "y = x ** 2\n",
    "y.retain_grad() # 加上这个后，z对x进行求导时候也能顺带保存z对y求导的中间结果，没有这句，不能y.grad\n",
    "z = y ** 2\n",
    "y.requires_grad # True\n",
    "z.requires_grad # True\n",
    "z.backward() # 反向传播，就是z对最底层的x求导，结果再代入x的值的最终结果，这个本身不返回什么\n",
    "x.grad # 紧接着可以这样来查看刚才计算的结果，这个是用来看x的求导结果的\n",
    "y.grad # 作为中间变量的y，在z对x求导的过程中，只有在y.retain_grad()被设置才可以\n",
    "# y.backward() # y也是可以对x求梯度的\n",
    "# 切记，一个计算图只能backward一次，比如z.backward()之后，就不能再y.backward()了，如下：\n",
    "x = torch.arange(4., requires_grad = True)\n",
    "y = torch.dot(x, x)\n",
    "z = y ** 2 # xyz属于一个计算图\n",
    "y.backward() # y先进行梯度下降\n",
    "print(x.grad) \n",
    "z.backward() # 这时候会报错，因为这个计算图已经计算过梯度了\n",
    "\n",
    "# 改成下面这样可避免问题\n",
    "x = torch.arange(4., requires_grad = True)\n",
    "y = torch.dot(x, x)\n",
    "z = x.sum() # xy和xz属于两个不同的计算图\n",
    "y.backward() # y先进行梯度下降\n",
    "print(x.grad) \n",
    "z.backward() # 正常计算\n",
    "\n",
    "# 继续引出手动梯度清零：上面结果中的第二个x.grad的输出值不是z对x求导代入后的值，而是这个值加上了之前y对x求导得到的值，x梯度会默认累加\n",
    "x.grad.zero_() # 将过往累加的梯度清零，重新开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度模型训练/预测流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'train_auc': 0.6890240560991914, 'val_auc': 0.6876221862538593}\n",
      "{'epoch': 20, 'train_auc': 0.6906825378883971, 'val_auc': 0.6894832598913301}\n",
      "{'epoch': 30, 'train_auc': 0.6936672061619087, 'val_auc': 0.6930791091016655}\n",
      "{'epoch': 40, 'train_auc': 0.6913650786015225, 'val_auc': 0.6891752669335156}\n",
      "EarlyStopping!!!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load dl_train_module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore') # 关闭警告\n",
    "\n",
    "def create_dataloader(X, y, batch_size=1, shuffle=True): # 创建batch迭代器函数\n",
    "    torch_dataset = TensorDataset(X, y) # 创建数据集，必须是tensor类型\n",
    "    loader = DataLoader(dataset=torch_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_loss(task): # 获取损失函数\n",
    "    if task == \"binary\":\n",
    "        criterion = torch.nn.BCELoss() # 注：此损失函数要求：1.每个样本只能有一个概率值，即输出是1 dim的tensor；2.要求标签是float类型\n",
    "    elif task == \"multiclass\":\n",
    "        criterion = torch.nn.CrossEntropyLoss() # 注：此损失函数要求：1.每个样本必须有每个类别的概率，即便是2分类，即输出是2 dim的tensor；2.要求标签是long类型；3.标签是1维数据，内容是0/1/2/......，从0开始的类别索引，不是从1开始，也不是多维onehot。\n",
    "    elif task == \"regression_1\": # 回归类损失函数的标签形状要求和二分类的情况一致\n",
    "        criterion = torch.nn.L1Loss()\n",
    "    elif task == \"regression_2\":\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct value!!!\")\n",
    "    return criterion\n",
    "\n",
    "\n",
    "def get_optimizer(params, opt_criterion, learning_rate, l2): # 获取梯度优化器函数\n",
    "    if learning_rate <= 0 or l2 < 0: \n",
    "        raise ValueError(\"Please input correct learning_rate and l2!!!\")\n",
    "    if opt_criterion.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(params, lr=learning_rate, weight_decay=l2)\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct opt_criterion!!!\")\n",
    "    return optimizer\n",
    "\n",
    "def weight_init(m): # 网络参数初始化函数\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "\n",
    "def seed_torch(seed=2022): # 固定所有随机种子函数\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "\n",
    "# 预测/验证函数\n",
    "def predict_model(model, test_X, test_y = None, batch_size=1, task=None, metrics=None, device=\"cpu\"): # 有test_y时验证，没有时预测\n",
    "    test_loader = create_dataloader(test_X, test_y, batch_size=batch_size, shuffle=False)\n",
    "    loss_func = get_loss(task)\n",
    "    model = model.eval()\n",
    "    pred_ans, loss = [], 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device).squeeze() # 真实标签不管什么时候都需要是1dim tensor数据\n",
    "            target = target.long() if task == \"multiclass\" else target.float() # 但数据类型需要灵活，多分类时需要是long，其他时候都是float。\n",
    "            output = model(data).squeeze(dim=-1) # 输出结果一定在最后一个维度处压缩一下，二分类和回归对此有要求，需要1维数据；而多分类输出虽然不是1dim，但它的最后一个维度不是1，所以不会受到这个压缩的影响，不会出错。\n",
    "            pred_ans.append(output.cpu().data.numpy()) # 如果要将tensor类型转为numpy类型，那么该数据必须要存在cpu上才行！\n",
    "            loss += loss_func(output, target).item()\n",
    "    y_pred = np.concatenate(pred_ans).astype(\"float64\")\n",
    "    \n",
    "    if test_y is not None:\n",
    "        metrics_d = {}\n",
    "        for i in metrics:\n",
    "            if \"auc\" in i:\n",
    "                metrics_d[\"auc\"] = roc_auc_score(test_y.squeeze().data.numpy(), y_pred) # roc_auc_score的真实标签必须在前面\n",
    "            elif \"loss\" in i:\n",
    "                metrics_d[\"loss\"] = loss/len(y_pred)\n",
    "        return metrics_d\n",
    "    else:\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def train_model(model, X, y, valid_data=None, valid_split=0., batch_size=1, opt_criterion=\"adam\", task=\"binary\", \\\n",
    "                metrics=[\"loss\",\"auc\"], eval_metric=\"auc\", epochs=100, early_stopping_bounds=None, seed=2022, \\\n",
    "                learning_rate=0.01, l2=0.01, shuffle=True, save_path=None, device=\"cpu\", verbose=0, is_plt=True):\n",
    "    \n",
    "    # 固定随机数种子\n",
    "    seed_torch(seed=seed)\n",
    "    \n",
    "    # 模型设置\n",
    "    model = model.to(device) # 将参数部署到指定设备\n",
    "    model.apply(weight_init) # 初始化参数\n",
    "    loss_func = get_loss(task) # 获取损失函数\n",
    "    optimizer = get_optimizer(model.parameters(), opt_criterion, learning_rate, l2) # 获取梯度优化器\n",
    "    \n",
    "    # 数据设置\n",
    "    X, y = torch.tensor(X), torch.tensor(y).unsqueeze(dim=1) # 将numpy的输入转为tensor，记得要将标签升维用以切分数据集，后面再降为1dim\n",
    "    if valid_data and len(valid_data) == 2: # 优先自主设置验证集\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        val_X, val_y = valid_data[0], valid_data[1]\n",
    "        valid_loader = create_dataloader(val_X, val_y, batch_size=batch_size, shuffle=shuffle)\n",
    "    elif 1 > valid_split > 0: # 从数据中拆分验证集\n",
    "        len_data = list(range(X.shape[0]))\n",
    "        np.random.shuffle(len_data)\n",
    "        train_index, valid_index = len_data[:int((1-valid_split)*X.shape[0])], len_data[int((1-valid_split)*X.shape[0]):] # 获取训练集和验证集各自索引列表\n",
    "        X, val_X, y, val_y = X[train_index], X[valid_index], y[train_index], y[valid_index]\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        valid_loader = create_dataloader(val_X, val_y, batch_size=batch_size, shuffle=shuffle)\n",
    "    elif valid_split == 0: # 如果不设置验证集\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct valid_dataset!!!\")\n",
    "\n",
    "    # 训练模型\n",
    "    metrics_total = [] # 所有迭代次数指标结果集合\n",
    "    metrics_dict = {} # 每个epoch指标结果\n",
    "    if eval_metric or save_path: # 如果使用早停或者要保存参数\n",
    "        if eval_metric not in [\"auc\", \"loss\", None]:\n",
    "                raise ValueError(\"Please input correct eval_metric!!!!\")\n",
    "        threshold = float(\"inf\") # 早停阈值（实时更新）\n",
    "        cnt = 0 # 早停计数器\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        metrics_dict[\"epoch\"] = epoch\n",
    "        \n",
    "        # 训练\n",
    "        model = model.train()\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device).squeeze()\n",
    "            target = target.long() if task == \"multiclass\" else target.float()\n",
    "            output = model(data).squeeze(dim=-1) # 如果是二分类或回归，输出必须是1dim，如果是多分类，dim=-1不会改变输出\n",
    "            loss = loss_func(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) # 梯度裁剪，参数介绍：参数集合；最大梯度范数；梯度范数类型\n",
    "            optimizer.step()\n",
    "            \n",
    "        # 训练集指标获取\n",
    "        metric_train_dict = predict_model(model, X, y, batch_size=batch_size, task=task, metrics=metrics, device=device)\n",
    "        for i in metric_train_dict: metrics_dict[\"train_\"+i] = metric_train_dict[i]\n",
    "        if not valid_loader: \n",
    "            if verbose > 0 and epoch % verbose == 0: print(metrics_dict) # 如果不需要验证直接输出指标结果\n",
    "            metrics_total.append(metrics_dict) # 记录指标结果\n",
    "        \n",
    "        # 验证\n",
    "        if valid_loader: # 如果需要验证\n",
    "            # 验证集指标获取\n",
    "            metric_val_dict = predict_model(model, val_X, val_y, batch_size=batch_size, task=task, metrics=metrics, device=device)\n",
    "            for i in metric_val_dict: metrics_dict[\"val_\"+i] = metric_val_dict[i]\n",
    "            if verbose > 0 and epoch % verbose == 0: print(metrics_dict)\n",
    "            metrics_total.append(metrics_dict) # 记录指标结果\n",
    "            \n",
    "            if eval_metric or save_path:        \n",
    "                val_metric = metrics_dict[\"val_\"+eval_metric] if eval_metric == \"loss\" else -metrics_dict[\"val_\"+eval_metric]\n",
    "                if val_metric < threshold:\n",
    "                    threshold, cnt = val_metric, 0\n",
    "                    if save_path:\n",
    "                        torch.save(model.state_dict(), save_path) # 载入模型参数时：model = Network().load_state_dict(torch.load(path)), Network是该自定义的网络\n",
    "                        print(\"model is saved\")\n",
    "                elif val_metric >= threshold and eval_metric:\n",
    "                    cnt += 1\n",
    "                    if cnt > early_stopping_bounds: \n",
    "                        print(\"EarlyStopping!!!\")\n",
    "                        break\n",
    "    \n",
    "        \n",
    "    # 指标可视化\n",
    "    if is_plt:\n",
    "        metrics_total_dict = defaultdict(list)\n",
    "        for m_dict in metrics_total:\n",
    "            for key in m_dict:\n",
    "                if key != \"epoch\": metrics_total_dict[key].append(m_dict[key])\n",
    "        metrics_total_list = list(metrics_total_dict.values())\n",
    "        metrics_total_names = list(metrics_total_dict.keys())\n",
    "        fig, ax = plt.subplots(len(metrics_total_list)//2, figsize=(15,10))\n",
    "        if not isinstance(ax, np.ndarray): ax = [ax]\n",
    "        for i in range(len(ax)):\n",
    "            train_data, valid_data = metrics_total_list[i], metrics_total_list[i+2 if len(ax) == 2 else -1]\n",
    "            train_name, valid_name = metrics_total_names[i], metrics_total_names[i+2 if len(ax) == 2 else -1]\n",
    "            ax[i].plot(range(1, len(train_data)+1), train_data, label=train_name)\n",
    "            ax[i].plot(range(1, len(valid_data)+1), valid_data, label=valid_name)\n",
    "            ax[i].set_xlabel('epoch')\n",
    "            ax[i].grid(True)\n",
    "            ax[i].legend()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 获取数据\n",
    "    df = pd.read_csv(\"./dataset/binary_practice_data.csv\")\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    fea_names = list(X.columns)\n",
    "    X_np, y_np = np.array(X), np.array(y)\n",
    "    X_tensor, y_tensor = torch.tensor(np.array(X)).float(), torch.tensor(np.array(y)).float().unsqueeze(dim=1)\n",
    "    \n",
    "    # 定义网络\n",
    "    class Net(nn.Module): # 定义网络结构\n",
    "        def __init__(self, dim=10):\n",
    "            super(Net, self).__init__()\n",
    "            self.f = nn.Sequential(nn.Linear(dim, dim), nn.Linear(dim, dim), nn.Linear(dim, dim//2), \\\n",
    "                                   nn.Linear(dim//2, dim//2//2), nn.Linear(dim//2//2, 1))\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        def forward(self, x):\n",
    "            return F.sigmoid(self.dropout(self.f(x)))\n",
    "    # 开始训练\n",
    "    model = Net(dim=49)\n",
    "    model1 = train_model(model, X_np, y_np, valid_data=None, valid_split=0.2, batch_size=128, opt_criterion=\"adam\", task=\"binary\", \\\n",
    "                metrics=[\"auc\"], eval_metric=\"auc\", epochs=100, early_stopping_bounds=10, \\\n",
    "                learning_rate=0.01, l2=0.01, shuffle=True, save_path=None, device=\"cpu\", verbose=10, is_plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 循环网络介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些注意的点，下面代码暂时没有涉及：\n",
    "1.训练时每次epoch都要使用mini-batch数据\n",
    "2.注意每次输入和输出的shape以及对应的数据类型\n",
    "3.每个epoch后分别记录一次训练集和验证集的loss和ACC、AUC等评分指标，并最终画图展示以观察拟合情况（过拟合or欠拟合）\n",
    "4.由于模型涉及随机初始化和随机GD，相同模型和数据重复结果未必相同，所以要重复执行整个流程多次，叠加多个图＆平均多个结果。\n",
    "5.LSTM很容易过拟合，dropout是必要的步骤，可以在embedding后和LSTM层后都加入\n",
    "6.模型的复杂度要和数据量相适应！\n",
    "7.批量归一化也是比较重要的\n",
    "8.设置早停\n",
    "更多内容见：https://blog.csdn.net/fu_jian_ping/article/details/109147133?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164423947516781683998851%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164423947516781683998851&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-109147133.first_rank_v2_pc_rank_v29&utm_term=LSTM%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4&spm=1018.2226.3001.4187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_inital' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-27a2a982e487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 跑模型，得到每个时点的预测结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 计算损失函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-27a2a982e487>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# hidden_inital是初始存储状态，可不写，那样默认全是0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_inital\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 套输出层\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 把输出结果降维\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_inital' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "num_time_steps = 50\n",
    "start = np.random.randint(3, size = 1)[0]\n",
    "time_steps = np.linspace(start, start + 10, num_time_steps)\n",
    "data = np.sin(time_steps).reshape(num_time_steps, 1)\n",
    "x = torch.tensor(data[:-1]).float().view(1, num_time_steps-1, 1)\n",
    "y = torch.tensor(data[1:]).float().view(1, num_time_steps-1, 1)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size): # 参数设置别忘了（这些参数不会默认传给forward函数）\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size, # 特征维度\n",
    "            hidden_size = hidden_size, # 隐层神经元数\n",
    "            num_layers = num_layers, # 隐层数\n",
    "            batch_first = True # 是否让batch参数总是在第一位\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size) # 输出线性层，如果是分类问题这里的output_size应该是类别数，且要再下面加一层softmax\n",
    "        \n",
    "    def forward(self, x, hidden_inital): # hidden_inital是初始存储状态，可不写，那样默认全是0\n",
    "        out, hidden_final = self.rnn(x, hidden_inital) # 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\n",
    "        out = self.linear(out) # 套输出层\n",
    "        out = out.squeeze(dim = 0) # 把输出结果降维\n",
    "        return out, hidden_final\n",
    "\n",
    "def xavier(m): # 参数Xavier初始化\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "model = Net(1, 20, 1, 1) # 创建网络（别忘了输入参数）\n",
    "model.apply(xavier) # 应用初始化参数\n",
    "criterion = nn.MSELoss() # 损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.03) # 梯度优化器\n",
    "hidden_inital = torch.zeros(1, 1, 20) # 初始隐层状态设置（非必须项）\n",
    "\n",
    "epoch = 6\n",
    "for iter in range(epoch):\n",
    "    output, hidden_final = model(x, hidden_inital) # 跑模型，得到每个时点的预测结果\n",
    "    hidden_final = hidden_final.detach()\n",
    "    loss = criterion(output, y) # 计算损失函数\n",
    "    optimizer.zero_grad() # 清零过往参数的梯度结果\n",
    "    loss.backward() # 根据损失函数值计算梯度\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) # 梯度裁剪，参数介绍：参数集合；最大梯度范数；梯度范数类型\n",
    "    optimizer.step() # 根据梯度更新参数\n",
    "    if iter % 100 == 0: print(\"Iteration: {} loss {}\".format(iter, loss.item())) # 每100次迭代返回一次结果\n",
    "\n",
    "pred = [] # 每个时点的预测结果\n",
    "test_x = x[:, 0, :]\n",
    "for i in range(x.shape[1]):\n",
    "    test_x = test_x.view(1,1,1)\n",
    "    pre, h = model(test_x, hidden_inital)\n",
    "    test_x = pre\n",
    "    pred.append(pre.squeeze().detach())\n",
    "x = x.data.numpy().ravel()\n",
    "y = y.data.numpy()\n",
    "plt.scatter(time_steps[:-1], x.squeeze())\n",
    "plt.plot(time_steps[:-1], x.ravel())\n",
    "plt.scatter(time_steps[:-1], pred)\n",
    "plt.show()\n",
    "\n",
    "# 模型存储\n",
    "# torch.save(model.state_dict(), \"simple_net\") # 存储训练好的模型参数\n",
    "# model1 = Net() # 加载的时候自行写模型\n",
    "# model1.load_state_dict(torch.load(\"simple_net\")) # 加载模型参数\n",
    "# model1.eval() # 固定非参数层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size,vocab_size. embedding_dim): # 参数设置别忘了（这些参数不会默认传给forward函数）\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size. embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size, # 特征维度\n",
    "            hidden_size = hidden_size, # 隐层神经元数\n",
    "            num_layers = num_layers, # 隐层数\n",
    "            batch_first = True # 是否让batch参数总是在第一位\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size) # 输出线性层，如果是分类问题这里的output_size应该是类别数，且要再下面加一层softmax\n",
    "        \n",
    "    def forward(self, x, hidden_inital): # hidden_inital是初始存储状态，可不写，那样默认全是0\n",
    "        out, hidden_final = self.rnn(x, hidden_inital) # 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\n",
    "        out = self.linear(out) # 套输出层\n",
    "        out = out.unsqueeze(dim = 0) # 把输出结果降维\n",
    "        return out, hidden_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5131, -0.9116],\n",
       "         [-0.7529, -2.9124],\n",
       "         [ 0.5857,  0.1588]],\n",
       "\n",
       "        [[ 0.0643, -0.1037],\n",
       "         [-0.3135,  1.4883],\n",
       "         [-0.6787,  0.0332]],\n",
       "\n",
       "        [[ 0.2943,  2.2066],\n",
       "         [ 0.3448,  0.5130],\n",
       "         [ 0.7544, -2.6062]],\n",
       "\n",
       "        [[ 0.3048, -0.1155],\n",
       "         [-1.3388, -1.2959],\n",
       "         [-1.1404, -0.4734]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(3,2)\n",
    "x = torch.tensor([[0,0],[1,1],[2,2]])\n",
    "x = torch.randn(4,3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0713],\n",
       "         [ 0.8296],\n",
       "         [-0.7156]],\n",
       "\n",
       "        [[-0.4200],\n",
       "         [ 0.7204],\n",
       "         [ 0.5326]],\n",
       "\n",
       "        [[ 0.0787],\n",
       "         [-0.1089],\n",
       "         [ 0.5416]],\n",
       "\n",
       "        [[ 0.2934],\n",
       "         [-0.5873],\n",
       "         [ 0.8578]],\n",
       "\n",
       "        [[-0.3617],\n",
       "         [-0.8861],\n",
       "         [-0.6540]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(2, 1)\n",
    "x = torch.randn(5, 3, 2)\n",
    "linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0516, -0.0358],\n",
       "         [ 1.0483, -0.6792],\n",
       "         [ 0.2636,  1.3180]],\n",
       "\n",
       "        [[-0.7964, -0.2332],\n",
       "         [ 0.1738, -1.3295],\n",
       "         [ 0.2012, -0.9681]],\n",
       "\n",
       "        [[ 1.8757,  1.4593],\n",
       "         [-0.2722, -0.2815],\n",
       "         [ 0.9224, -0.2873]],\n",
       "\n",
       "        [[ 0.6077, -0.1488],\n",
       "         [-0.2370,  0.6056],\n",
       "         [ 0.9523, -0.8223]],\n",
       "\n",
       "        [[ 0.8626,  1.2658],\n",
       "         [-0.7258,  0.6662],\n",
       "         [-1.1668, -0.1738]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7156,  0.5326,  0.5416,  0.8578, -0.6540], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x.reshape(-1,2)\n",
    "linear(x1).reshape(5,-1)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7156,  0.5326,  0.5416,  0.8578, -0.6540],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x[:,-1:,:].squeeze()\n",
    "linear(x2).squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer源码(仅供参考）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写框架的时候两个要点：\n",
    "- 先写大框架，再写小框架，一级一级往下写\n",
    "- 不确定的输入形式可以先不写，后面补上\n",
    "- 时刻清楚数据在每个部分流转时候的形态和类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.280869\n",
      "Epoch: 0002 cost = 3.970184\n",
      "Epoch: 0003 cost = 2.795365\n",
      "Epoch: 0004 cost = 5.159268\n",
      "Epoch: 0005 cost = 3.878076\n",
      "Epoch: 0006 cost = 5.810416\n",
      "Epoch: 0007 cost = 5.532382\n",
      "Epoch: 0008 cost = 4.226982\n",
      "Epoch: 0009 cost = 5.170188\n",
      "Epoch: 0010 cost = 1.904359\n",
      "Epoch: 0011 cost = 2.123083\n",
      "Epoch: 0012 cost = 2.040266\n",
      "Epoch: 0013 cost = 2.028134\n",
      "Epoch: 0014 cost = 2.044340\n",
      "Epoch: 0015 cost = 1.865151\n",
      "Epoch: 0016 cost = 1.688594\n",
      "Epoch: 0017 cost = 1.676789\n",
      "Epoch: 0018 cost = 1.732380\n",
      "Epoch: 0019 cost = 1.743735\n",
      "Epoch: 0020 cost = 1.714740\n",
      "ich mochte ein bier P -> ['E', 'E', 'E', 'E', 'E']\n",
      "first head of last state enc_self_attns\n"
     ]
    }
   ],
   "source": [
    "# code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
    "# Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "#           https://github.com/JayParks/transformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder() # 定义encoder类\n",
    "        self.decoder = Decoder() # 定义Decoder类\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False) # \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__': # 框架搭建完毕，开始使用\n",
    "    # 数据集，只有一个样本，包含编码器输入，解码器输入和真实标签三部分\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E'] # P是填充字符；S是开始字符；E是结束字符\n",
    "    src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4} # 编码端字典\n",
    "    src_vocab_size = len(src_vocab) # 字典长度\n",
    "\n",
    "    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6} # 解码端字典，一般与编码端是一样的\n",
    "#     number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "    tgt_vocab_size = len(tgt_vocab) # 解码端字典长度\n",
    "\n",
    "    src_len = 5 # length of source # 编码端输入句子长度\n",
    "    tgt_len = 5 # length of target # 解码端输入句子长度\n",
    "\n",
    "    d_model = 512  # Embedding Size # 输入向量经过embedding层后输入的维度\n",
    "    d_ff = 2048  # FeedForward dimension # 全连接层的维度\n",
    "    d_k = d_v = 64  # dimension of K(=Q), V # 输入转化为query,key,value向量的维度\n",
    "    n_layers = 6  # number of Encoder of Decoder Layer # 编码和解码器中block的数量\n",
    "    n_heads = 8  # number of heads in Multi-Head Attention # 多头注意力机制的数量\n",
    "\n",
    "    model = Transformer() # 定义transformer层\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # 定义损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # 定义迭代优化器\n",
    "\n",
    "    # 数据处理，将文本型数据进行分词，并转为在字典中对应的索引，这里由于只有一个样本，就没使用迭代器\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences) # 输出三个部分，分别是编码器、解码器和标签的索引LongTenor列表\n",
    "\n",
    "    for epoch in range(20): # 开始迭代\n",
    "        optimizer.zero_grad() # 参数梯度清零\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs) # 输入输出\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1)) # 计算损失函数\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) # 打印迭代结果\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "    # Test\n",
    "    predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    print('first head of last state enc_self_attns')\n",
    "    showgraph(enc_self_attns)\n",
    "\n",
    "    print('first head of last state dec_self_attns')\n",
    "    showgraph(dec_self_attns)\n",
    "\n",
    "    print('first head of last state dec_enc_attns')\n",
    "    showgraph(dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数据预处理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 使用gensim处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: [['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "texts1: [['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "ID:  {'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "Freq:  {1: 2, 2: 2, 0: 2, 4: 2, 7: 3, 5: 3, 3: 2, 6: 2, 8: 2, 9: 3, 10: 3, 11: 2}\n",
      "\n",
      "corpus: [[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n",
      "TFIDF矩阵: [[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)], [(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)], [(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)], [(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)], [(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)], [(9, 1.0)], [(9, 0.7071067811865475), (10, 0.7071067811865475)], [(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)], [(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.7702928e-03,  8.1651136e-03,  1.2809705e-03,  5.0975773e-03,\n",
       "         1.4081288e-03, -6.4551616e-03, -1.4280510e-03,  6.4491653e-03,\n",
       "        -4.6173073e-03, -3.9930656e-03,  4.9244044e-03,  2.7130984e-03,\n",
       "        -1.8479753e-03, -2.8769446e-03,  6.0107303e-03, -5.7167388e-03,\n",
       "        -3.2367038e-03, -6.4878250e-03, -4.2346334e-03, -8.5809948e-03,\n",
       "        -4.4697905e-03, -8.5112313e-03,  1.4037776e-03, -8.6181974e-03,\n",
       "        -9.9166557e-03, -8.2016252e-03, -6.7726658e-03,  6.6805840e-03,\n",
       "         3.7845564e-03,  3.5616636e-04, -2.9579829e-03, -7.4283220e-03,\n",
       "         5.3341867e-04,  4.9989222e-04,  1.9561767e-04,  8.5259438e-04,\n",
       "         7.8633073e-04, -6.8161491e-05, -8.0070542e-03, -5.8702733e-03,\n",
       "        -8.3829118e-03, -1.3120436e-03,  1.8206357e-03,  7.4171280e-03,\n",
       "        -1.9634271e-03, -2.3252917e-03,  9.4871549e-03,  7.9703328e-05,\n",
       "        -2.4045228e-03,  8.6048460e-03,  2.6870037e-03, -5.3439736e-03,\n",
       "         6.5881060e-03,  4.5101522e-03, -7.0544672e-03, -3.2317400e-04,\n",
       "         8.3448651e-04,  5.7473565e-03, -1.7176557e-03, -2.8065301e-03,\n",
       "         1.7484308e-03,  8.4717036e-04,  1.1928272e-03, -2.6342822e-03,\n",
       "        -5.9857843e-03,  7.3229838e-03,  7.5873756e-03,  8.2963565e-03,\n",
       "        -8.5988473e-03,  2.6364254e-03, -3.5599638e-03,  9.6204039e-03,\n",
       "         2.9037667e-03,  4.6411133e-03,  2.3856140e-03,  6.6084769e-03,\n",
       "        -5.7432912e-03,  7.8944108e-03, -2.4109220e-03, -4.5618867e-03,\n",
       "        -2.0609903e-03,  9.7335577e-03, -6.8565919e-03, -2.1917201e-03,\n",
       "         7.0009995e-03, -5.5749417e-05, -6.2949681e-03, -6.3935257e-03,\n",
       "         8.9403940e-03,  6.4295745e-03,  4.7735930e-03, -3.2620477e-03,\n",
       "        -9.2676207e-03,  3.7868882e-03,  7.1605491e-03, -5.6328895e-03,\n",
       "        -7.8650145e-03, -2.9727411e-03, -4.9318983e-03, -2.3151112e-03],\n",
       "       [ 7.0871473e-03, -1.5683782e-03,  7.9461364e-03, -9.4874427e-03,\n",
       "        -8.0296379e-03, -6.6422895e-03, -4.0041069e-03,  4.9910326e-03,\n",
       "        -3.8136279e-03, -8.3215833e-03,  8.4132208e-03, -3.7471871e-03,\n",
       "         8.6089820e-03, -4.8962692e-03,  3.9189272e-03,  4.9236091e-03,\n",
       "         2.3943025e-03, -2.8213640e-03,  2.8496354e-03, -8.2571078e-03,\n",
       "        -2.7652937e-03, -2.5905678e-03,  7.2491039e-03, -3.4629786e-03,\n",
       "        -6.5992209e-03,  4.3399399e-03, -4.7540484e-04, -3.5954388e-03,\n",
       "         6.8824128e-03,  3.8715161e-03, -3.8985382e-03,  7.7237905e-04,\n",
       "         9.1438834e-03,  7.7564754e-03,  6.3607404e-03,  4.6693156e-03,\n",
       "         2.3858545e-03, -1.8414279e-03, -6.3720327e-03, -2.9998334e-04,\n",
       "        -1.5642069e-03, -5.7058921e-04, -6.2630447e-03,  7.4330512e-03,\n",
       "        -6.5910248e-03, -7.2386782e-03, -2.7575779e-03, -1.5144736e-03,\n",
       "        -7.6353899e-03,  6.9955905e-04, -5.3253169e-03, -1.2739648e-03,\n",
       "        -7.3665036e-03,  1.9612678e-03,  3.2734126e-03, -2.4741203e-05,\n",
       "        -5.4490739e-03, -1.7256952e-03,  7.0866016e-03,  3.7357579e-03,\n",
       "        -8.8817989e-03, -3.4116022e-03,  2.3567537e-03,  2.1376403e-03,\n",
       "        -9.4648376e-03,  4.5697815e-03, -8.6582452e-03, -7.3883547e-03,\n",
       "         3.4832379e-03, -3.4728234e-03,  3.5656209e-03,  8.8950340e-03,\n",
       "        -3.5753339e-03,  9.3197320e-03,  1.7111641e-03,  9.8462272e-03,\n",
       "         5.7051540e-03, -9.1497740e-03, -3.3269732e-03,  6.5306681e-03,\n",
       "         5.6031067e-03,  8.7057864e-03,  6.9263605e-03,  8.0408510e-03,\n",
       "        -9.8231193e-03,  4.2975070e-03, -5.0308690e-03,  3.5132242e-03,\n",
       "         6.0578999e-03,  4.3924102e-03,  7.5100451e-03,  1.4993446e-03,\n",
       "        -1.2642510e-03,  5.7697659e-03, -5.6368350e-03,  3.7106704e-05,\n",
       "         9.4556073e-03, -5.4809595e-03,  3.8159816e-03, -8.1130704e-03],\n",
       "       [-5.1577436e-03, -6.6702794e-03, -7.7790986e-03,  8.3131464e-03,\n",
       "        -1.9829201e-03, -6.8569588e-03, -4.1555981e-03,  5.1456233e-03,\n",
       "        -2.8699716e-03, -3.7507527e-03,  1.6218971e-03, -2.7771019e-03,\n",
       "        -1.5848217e-03,  1.0748033e-03, -2.9788127e-03,  8.5217617e-03,\n",
       "         3.9120731e-03, -9.9617615e-03,  6.2614209e-03, -6.7562214e-03,\n",
       "         7.6965551e-04,  4.4055167e-03, -5.1048612e-03, -2.1112841e-03,\n",
       "         8.0978349e-03, -4.2450288e-03, -7.6384838e-03,  9.2606070e-03,\n",
       "        -2.1561245e-03, -4.7208075e-03,  8.5732946e-03,  4.2845840e-03,\n",
       "         4.3260953e-03,  9.2872158e-03, -8.4555410e-03,  5.2568498e-03,\n",
       "         2.0399450e-03,  4.1894992e-03,  1.6983944e-03,  4.4654319e-03,\n",
       "         4.4875946e-03,  6.1062989e-03, -3.2030295e-03, -4.5770612e-03,\n",
       "        -4.2664065e-04,  2.5344712e-03, -3.2641180e-03,  6.0594808e-03,\n",
       "         4.1553383e-03,  7.7668522e-03,  2.5700203e-03,  8.1190439e-03,\n",
       "        -1.3876136e-03,  8.0802785e-03,  3.7180968e-03, -8.0496678e-03,\n",
       "        -3.9347606e-03, -2.4725979e-03,  4.8944671e-03, -8.7241287e-04,\n",
       "        -2.8317324e-03,  7.8359870e-03,  9.3256142e-03, -1.6153983e-03,\n",
       "        -5.1607513e-03, -4.7031282e-03, -4.8474614e-03, -9.6056210e-03,\n",
       "         1.3724195e-03, -4.2261453e-03,  2.5274432e-03,  5.6161173e-03,\n",
       "        -4.0670903e-03, -9.5993746e-03,  1.5471478e-03, -6.7020725e-03,\n",
       "         2.4959005e-03, -3.7817324e-03,  7.0804814e-03,  6.4040697e-04,\n",
       "         3.5619752e-03, -2.7399310e-03, -1.7110463e-03,  7.6550203e-03,\n",
       "         1.4080878e-03, -5.8521517e-03, -7.8367768e-03,  1.2330448e-03,\n",
       "         6.4565083e-03,  5.5579669e-03, -8.9796642e-03,  8.5946647e-03,\n",
       "         4.0481547e-03,  7.4717780e-03,  9.7491713e-03, -7.2917030e-03,\n",
       "        -9.0425937e-03,  5.8376994e-03,  9.3939463e-03,  3.5079459e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "\n",
    "# 去掉停用词\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "print(\"texts: {}\".format(texts))\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)   # 生成词典\n",
    "# 去掉出现少于两次的单词，去掉出现多于5次的单词，剩下的单词保留前100个\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 5, keep_n = 100)\n",
    "texts1 = [[word for word in text if word in dictionary.token2id] for text in texts] # 更新texts\n",
    "print(\"\\ntexts1: {}\".format(texts1))\n",
    "print(\"\\nID: \", dictionary.token2id) # diction.token2id 存放的是单词-id key-value对\n",
    "print(\"Freq: \", dictionary.dfs) # diction.dfs 存放的是单词的出现频率\n",
    "\n",
    "# 1. 下面是用TF-IDF模型进行编码\n",
    "# 参考资料1：https://zhuanlan.zhihu.com/p/37175253\n",
    "# 参考资料2：https://www.cnblogs.com/keye/p/9190304.html\n",
    "\n",
    "# 建立语料库\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # 将文本数据转为id和频率数据（主要看dic的状态，texts不更新结果也一样）\n",
    "print(\"\\ncorpus: {}\".format(corpus))\n",
    "\n",
    "# 建立TF-IDF模型并生成矩阵\n",
    "tfidf_model = models.TfidfModel(corpus) # 训练TF-IDF模型，这个模型可以保存，详情见参考资料\n",
    "corpus_tfidf = tfidf_model[corpus] # 进行预测，获得TF-IDF矩阵，但是是个迭代器类型，只能用for来把每个样本的向量取出\n",
    "# 注意：上面这个预测过程不仅仅可以预测原数据，也可以预测别的数据，只要保证数据格式符合要求，且其映射成的字典需与模型训练集的相同\n",
    "print(\"TFIDF矩阵: {}\".format([v for v in corpus_tfidf]))\n",
    "\n",
    "# 2. 下面是用work2vec方法进行编码\n",
    "# 关于使用方法的详情，请见官方文档：https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = models.word2vec.Word2Vec(texts1, min_count=1) # 训练模型，输入的是分词的结果，参数细节见https://blog.csdn.net/u011748542/article/details/85880852\n",
    "model.wv.most_similar(\"user\",topn=10) # 计算训练集见过的词中与输入词的相似度，并降序排列取出前十个(输入词必须见过)\n",
    "model.wv.similarity('user', 'computer') # 计算两个输入词的相似性（输入词必须都见过）\n",
    "# model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) # 猜词，结果是queen，这个不使用前面的数据了\n",
    "model.wv[texts1[0]].mean(axis = 1) # 测试模型，输入的词必须是训练过的，这里输入第一个样本的分词结果，得到第一个样本的全部词向量，然后做平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用nn.embedding+预训练处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于深度学习NLP问题，纯词向量特征，每个词一个向量后pooling在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7],\n",
      "        [7]]), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "PackedSequence(data=tensor([[ 0.1443,  0.0625, -0.0232,  0.1436,  0.1275],\n",
      "        [ 0.2047,  0.1032,  0.0009,  0.2001,  0.1720],\n",
      "        [ 0.2918,  0.1329, -0.0132,  0.2646,  0.2305],\n",
      "        [ 0.3570,  0.1644,  0.0121,  0.3265,  0.2902],\n",
      "        [ 0.4013,  0.1807,  0.0043,  0.3498,  0.3163],\n",
      "        [ 0.4515,  0.1960,  0.0237,  0.3876,  0.3774],\n",
      "        [ 0.4726,  0.2066,  0.0198,  0.3947,  0.3901],\n",
      "        [ 0.5081,  0.2103,  0.0323,  0.4091,  0.4491],\n",
      "        [ 0.5179,  0.2178,  0.0305,  0.4110,  0.4556],\n",
      "        [ 0.5428,  0.2147,  0.0372,  0.4115,  0.5099],\n",
      "        [ 0.5474,  0.2205,  0.0364,  0.4120,  0.5132],\n",
      "        [ 0.5649,  0.2140,  0.0388,  0.4063,  0.5607],\n",
      "        [ 0.5669,  0.2188,  0.0385,  0.4065,  0.5624]], grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None) \n",
      "\n",
      "\n",
      "tensor([[[ 0.1443,  0.0625, -0.0232,  0.1436,  0.1275],\n",
      "         [ 0.2918,  0.1329, -0.0132,  0.2646,  0.2305],\n",
      "         [ 0.4013,  0.1807,  0.0043,  0.3498,  0.3163],\n",
      "         [ 0.4726,  0.2066,  0.0198,  0.3947,  0.3901],\n",
      "         [ 0.5179,  0.2178,  0.0305,  0.4110,  0.4556],\n",
      "         [ 0.5474,  0.2205,  0.0364,  0.4120,  0.5132],\n",
      "         [ 0.5669,  0.2188,  0.0385,  0.4065,  0.5624]],\n",
      "\n",
      "        [[ 0.2047,  0.1032,  0.0009,  0.2001,  0.1720],\n",
      "         [ 0.3570,  0.1644,  0.0121,  0.3265,  0.2902],\n",
      "         [ 0.4515,  0.1960,  0.0237,  0.3876,  0.3774],\n",
      "         [ 0.5081,  0.2103,  0.0323,  0.4091,  0.4491],\n",
      "         [ 0.5428,  0.2147,  0.0372,  0.4115,  0.5099],\n",
      "         [ 0.5649,  0.2140,  0.0388,  0.4063,  0.5607],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>) \n",
      "\n",
      "\n",
      "tensor([[[0.3482, 0.6518],\n",
      "         [0.3367, 0.6633],\n",
      "         [0.3292, 0.6708],\n",
      "         [0.3260, 0.6740],\n",
      "         [0.3256, 0.6744],\n",
      "         [0.3267, 0.6733],\n",
      "         [0.3285, 0.6715]],\n",
      "\n",
      "        [[0.3417, 0.6583],\n",
      "         [0.3319, 0.6681],\n",
      "         [0.3276, 0.6724],\n",
      "         [0.3266, 0.6734],\n",
      "         [0.3275, 0.6725],\n",
      "         [0.3292, 0.6708],\n",
      "         [0.3580, 0.6420]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "# 看下面内容前，一定先看参考资料：https://blog.csdn.net/lssc4205/article/details/79474735，弄清楚三个pad函数是干嘛的\n",
    "\n",
    "# 1. 假设我们已经完成了所有的分词工作，并且要使用预训练好的大型embedding矩阵进行编码\n",
    "# 注：（其实可以将nn.Embedding作为层放到网络中随模型一起进行有监督的训练来编码，那样最原始的输入不需要embedding，但暂时不讨论这个）\n",
    "# 2. 假设我们获得了大型embedding矩阵的词字典及每个词的索引位置，并将自己分好词的数据集中每个词转换为对应的词索引\n",
    "# 3. 文本转为索引后，经常面临不同batch维度不一致问题，导致数据不能输入模型，下面是一般的处理过程：\n",
    "\n",
    "# 3.1 首先把词索引矩阵list一下，变成下面的列表型，每个元素都是一个样本的词向量索引列表，然后按索引长度对样本降序排序：（降序的原因在后面）\n",
    "train_x = [torch.tensor([0, 1, 3]),\n",
    "           torch.tensor([2, 3]),\n",
    "           torch.tensor([3])]\n",
    "\n",
    "# 3.2 然后使用pad_sequence对长度不齐的部分进行数据补值，默认值自选padding_value\n",
    "train_x1 = nn.utils.rnn.pad_sequence(train_x, batch_first=True, padding_value=0) # 填充值可以随便选，后面不会被计算\n",
    "\n",
    "# 3.3 维度问题解决后，进行假装进行embedding\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], \n",
    "                            [4, 5.1, 6.3],\n",
    "                            [0.3,4.5,2.4],\n",
    "                            [3, 4, 5.0]]) # 假装weight就是大型矩阵\n",
    "embedding = nn.Embedding.from_pretrained(weight) # 直接承接预训练结果\n",
    "embedded = embedding(train_x1) # 对填充后的tensor列表进行embedding操作，结果集中原本不存在的部分也被编码了，但后面会去除\n",
    "\n",
    "# 3.4 关键步骤：对上述结果压缩为PackedSequence类型，第二个参数是每个样本的词个数组成的列表，必须用最开始数据的长度列表（压缩的原因见最后）\n",
    "padded_data = nn.utils.rnn.pack_padded_sequence(embedded, [len(data) for data in train_x], batch_first=True)\n",
    "# 解答：样本按词数降序排序的原因就是为了在告诉函数长度列表后，他能知道每个seq中哪些样本的词是被填补的词，这部分是不会输入模型的\n",
    "# 注意：其实到3.4为止，其结果就可以被RNN和LSTM给接受了（attention暂时不知)，可以直接输入模型，模型自己就知道该跑哪些部分，但是线性层不可以使用\n",
    "\n",
    "# 3.5 将数据转回tensor格式，所有填补的词的embedding向量在这里都是0向量\n",
    "packed_data = nn.utils.rnn.pad_packed_sequence(padded_data, batch_first=True) # 这个结果可以输入模型了\n",
    "# 注：压缩原因：一般为了避免不必要的存储和计算（填充词即便是0也会占资源，调用计算），在RNN和LSTM模型中，不把3.5后结果扔进去，而是用3.4后的结果，填充的部分自动忽略\n",
    "# 当从循环网络出来后接线性层时，再把得到的结果经过3.5过程转化为tensor；如果不是RNN或LSTM，就不一定能这样做了\n",
    "\n",
    "\n",
    "#  以下步骤待定，不一定采用，下次再写\n",
    "class MyData(Dataset.Dataset):\n",
    "    def __init__(self, train_x):\n",
    "        self.train_x = train_x\n",
    "    def __len__(self):\n",
    "        return len(self.train_x)\n",
    "    def __getitem__(self, item):\n",
    "        return self.train_x[item]\n",
    "\n",
    "def collate_fn(train_data):\n",
    "    train_data.sort(key = lambda data: len(data), reverse = True)\n",
    "    data_length = [len(data) for data in train_data]\n",
    "    train_data = nn.utils.rnn.pad_sequence(train_data, batch_first=True, padding_value=0) # padding过程\n",
    "    return train_data.unsqueeze(-1), data_length\n",
    "\n",
    "train_data = MyData(train_x)\n",
    "train_dataloader = DataLoader.DataLoader(train_data, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# padding后的结果\n",
    "# for data, length in train_dataloader:\n",
    "#     print(data)\n",
    "#     print(length)\n",
    "# padding后会有资源浪费，继续padded可以让每次循环输入后没有0填充值（必须再padding后操作）\n",
    "\n",
    "net = nn.LSTM(1, 5, batch_first=True) # 简单的模型\n",
    "Linear = nn.Linear(5, 2)\n",
    "for data, length in train_dataloader:\n",
    "    data = nn.utils.rnn.pack_padded_sequence(data, length, batch_first=True) # padded操作\n",
    "    print(data) # padded后的数据\n",
    "    output, hidden = net(data.float()) # 将padded后的数据输入模型得到output，这个padded后的输入仅能在LSTM或RNN层输入，不能输入普通线性层\n",
    "    print(output,\"\\n\\n\")\n",
    "    output, out_len = nn.utils.rnn.pad_packed_sequence(output, batch_first=True) # 将output转回正常形态以方便接下来接入线性层\n",
    "    print(output,\"\\n\\n\") # 普通状态的output展示\n",
    "    print(F.softmax(Linear(output), dim = 2)) # 输出概率结果\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "The 'sacremoses' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:105\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     got_ver \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib_metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\metadata.py:530\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\metadata.py:503\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\metadata.py:177\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: sacremoses",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     _LazyModule,\n\u001b[0;32m     46\u001b[0m     is_flax_available,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     is_vision_available,\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:120\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git master\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:107\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    105\u001b[0m     got_ver \u001b[38;5;241m=\u001b[39m importlib_metadata\u001b[38;5;241m.\u001b[39mversion(pkg)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m importlib_metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m importlib_metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError(\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m distribution was not found and is required by this application. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m     )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# check that the right version is installed if version number or a range was provided\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: The 'sacremoses' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
